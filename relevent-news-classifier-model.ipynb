{"metadata":{"kernelspec":{"name":"python385jvsc74a57bd0a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":[],"cell_type":"markdown","metadata":{}},{"source":["## The notebook which will make the model and export it in a pickle which will be later loaded by the web app"],"cell_type":"markdown","metadata":{}},{"source":["## Downloading popular nltk packages"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     C:\\Users\\aadit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","nltk.download('popular')"]},{"source":["## Importing needed libraries and loading the dataset"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pickle\n","df = pd.read_json('data/News_Category_Dataset.json', lines=True)"],"metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"source":["## Filtering the dataframe to get the data needed"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["df = df.filter(['category', 'headline'])\n","df"],"metadata":{"trusted":true},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             category                                           headline\n","0               CRIME  There Were 2 Mass Shootings In Texas Last Week...\n","1       ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...\n","2       ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57\n","3       ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n","4       ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...\n","...               ...                                                ...\n","200848           TECH  RIM CEO Thorsten Heins' 'Significant' Plans Fo...\n","200849         SPORTS  Maria Sharapova Stunned By Victoria Azarenka I...\n","200850         SPORTS  Giants Over Patriots, Jets Over Colts Among  M...\n","200851         SPORTS  Aldon Smith Arrested: 49ers Linebacker Busted ...\n","200852         SPORTS  Dwight Howard Rips Teammates After Magic Loss ...\n","\n","[200853 rows x 2 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>headline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>Hugh Grant Marries For The First Time At Age 57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>200848</th>\n      <td>TECH</td>\n      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n    </tr>\n    <tr>\n      <th>200849</th>\n      <td>SPORTS</td>\n      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n    </tr>\n    <tr>\n      <th>200850</th>\n      <td>SPORTS</td>\n      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n    </tr>\n    <tr>\n      <th>200851</th>\n      <td>SPORTS</td>\n      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n    </tr>\n    <tr>\n      <th>200852</th>\n      <td>SPORTS</td>\n      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>200853 rows Ã— 2 columns</p>\n</div>"},"metadata":{},"execution_count":3}]},{"source":["## Mapping the topics to its relevance"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["relevance = {\n","    'ARTS': 1,\n","    'ARTS & CULTURE': 0,\n","    'BLACK VOICES': 0,\n","    'BUSINESS': 1,\n","    'COLLEGE': 0,\n","    'COMEDY': 0,\n","    'CRIME': 0,\n","    'CULTURE & ARTS': 0,\n","    'DIVORCE': 0,\n","    'EDUCATION': 1,\n","    'ENTERTAINMENT': 0,\n","    'ENVIRONMENT': 1,\n","    'FIFTY': 0,\n","    'FOOD & DRINK': 1,\n","    'GOOD NEWS': 1,\n","    'GREEN': 1,\n","    'HEALTHY LIVING': 1,\n","    'HOME & LIVING': 1,\n","    'IMPACT': 0,\n","    'LATINO VOICES': 0,\n","    'MEDIA': 0,\n","    'MONEY': 1,\n","    'PARENTING': 0,\n","    'PARENTS': 0,\n","    'POLITICS': 1,\n","    'QUEER VOICES': 0,\n","    'RELIGION': 0,\n","    'SCIENCE': 1,\n","    'SPORTS': 1,\n","    'STYLE': 0,\n","    'STYLE & BEAUTY': 1,\n","    'TASTE': 1,\n","    'TECH': 1,\n","    'THE WORLDPOST': 0,\n","    'TRAVEL': 1,\n","    'WEDDINGS': 0,\n","    'WEIRD NEWS': 0,\n","    'WELLNESS': 1,\n","    'WOMEN': 0,\n","    'WORLD NEWS': 0,\n","    'WORLDPOST': 0,\n","}\n","df['relevance'] = df.category.map(relevance)\n","df"],"metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             category                                           headline  \\\n","0               CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n","1       ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n","2       ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n","3       ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n","4       ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n","...               ...                                                ...   \n","200848           TECH  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n","200849         SPORTS  Maria Sharapova Stunned By Victoria Azarenka I...   \n","200850         SPORTS  Giants Over Patriots, Jets Over Colts Among  M...   \n","200851         SPORTS  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n","200852         SPORTS  Dwight Howard Rips Teammates After Magic Loss ...   \n","\n","        relevance  \n","0               0  \n","1               0  \n","2               0  \n","3               0  \n","4               0  \n","...           ...  \n","200848          1  \n","200849          1  \n","200850          1  \n","200851          1  \n","200852          1  \n","\n","[200853 rows x 3 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>headline</th>\n      <th>relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>Hugh Grant Marries For The First Time At Age 57</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>200848</th>\n      <td>TECH</td>\n      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>200849</th>\n      <td>SPORTS</td>\n      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>200850</th>\n      <td>SPORTS</td>\n      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>200851</th>\n      <td>SPORTS</td>\n      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>200852</th>\n      <td>SPORTS</td>\n      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>200853 rows Ã— 3 columns</p>\n</div>"},"metadata":{},"execution_count":4}]},{"source":["## Function which tokenizes, stems and lemmatizes given string"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords \n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","ps = PorterStemmer()\n","\n","def extract_words(text):\n","    stop_words = set(stopwords.words('english')) \n","    word_tokens = word_tokenize(text.lower()) \n","    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n","    filtered_sentence = [] \n","    for w in word_tokens: \n","        if w not in stop_words: \n","            stemmed = ps.stem(w)\n","            lemmed = lemmatizer.lemmatize(stemmed)\n","            filtered_sentence.append(lemmed) \n","    return filtered_sentence"],"metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"source":["## Functions which will build the bagofwords"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def map_book(hash_map, tokens):\n","    if tokens is not None:\n","        for word in tokens:\n","            # Word Exist?\n","            if word in hash_map:\n","                hash_map[word] += 1\n","            else:\n","                hash_map[word] = 1\n","\n","        return hash_map\n","    else:\n","        return None\n","    \n","def make_hash_map(df):\n","    hash_map = {}\n","    for index, row in df.iterrows():\n","        hash_map = map_book(hash_map, extract_words(row['headline']))\n","    return hash_map\n","\n","def frequent_vocab(word_freq, max_features): \n","    counter = 0  \n","    vocab = [] \n","    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n","        if counter<max_features: \n","            vocab.append(key)\n","            counter+=1\n","        else: break\n","    return vocab\n","\n","def bagofwords(sentence, words):\n","    sentence_words = extract_words(sentence)\n","    bag = np.zeros(len(words))\n","    for sw in sentence_words:\n","        for i,word in enumerate(words):\n","            if word == sw: \n","                bag[i] += 1\n","                \n","    return np.array(bag)"],"metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"source":["## Make a variable which will hold the frequent vocabs in the hashmap\n","And dump it in a pickle file to be later loaded by the web app"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["hash_map = make_hash_map(df) \n","\n","vocab=frequent_vocab(hash_map, 500)\n","\n","print(vocab)\n","pickle.dump(vocab, open('web/vocab.pickle', 'wb'))"],"metadata":{"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[',', ':', \"'s\", \"'\", '(', ')', 'trump', 'photo', '?', 'new', \"n't\", 'video', 'say', 'â€™', 'day', 'make', 'get', 'woman', '.', 'donald', 'way', 'best', 'year', '5', 'show', 'world', '!', 'kid', 'time', 'one', '10', 'like', 'thing', 'love', 'peopl', 'look', 'want', 'need', 'week', 'life', 'take', 'help', 'first', 'obama', 'could', 'live', 'child', '--', 'u.s.', 'health', 'know', '&', 'wed', 'chang', 'man', 'parent', 'find', 'hous', 'american', 'home', 'call', 'clinton', 'mom', 'america', 'watch', 'back', 'food', 'may', 'go', 'state', 'black', 'u', 'report', 'gop', 'right', 'white', 'talk', 'work', 'famili', 'use', 'give', 'school', '$', 'celebr', 'top', 'studi', 'bill', 'babi', 'star', 'travel', 'presid', 'gay', 'plan', 'divorc', 'tip', 'kill', '7', 'hillari', 'good', 'polic', 'recip', 'â€˜', 'girl', \"'the\", 'republican', 'come', 'realli', '3', 'fight', 'attack', \"'re\", 'stop', 'democrat', '...', 'reason', 'citi', 'fashion', 'big', 'still', 'death', 'think', 'care', '6', 'war', 'beauti', 'style', 'win', 'marriag', 'face', 'open', 'york', 'stori', 'news', 'dress', 'elect', 'senat', 'power', 'learn', 'shoot', 'would', 'support', 'nation', 'see', 'court', 'better', 'tri', 'holiday', 'student', 'save', 'end', 'eat', 'cancer', 'men', 'tell', 'vote', 'ever', '8', 'let', 'reveal', '2', 'real', 'keep', 'meet', 'mother', 'campaign', 'summer', 'parti', 'climat', 'sleep', 'offic', 'art', 'million', 'john', 'dog', 'dad', 'game', 'turn', 'never', 'secret', 'season', 'final', 'ask', 'deal', '4', 'dead', 'idea', 'gun', 'fire', 'ban', 'share', 'run', 'part', '2012', 'protest', 'twitter', 'guid', 'teen', 'tweet', 'fall', 'die', 'question', 'wo', 'everi', 'break', 'movi', 'move', 'great', \"''\", 'busi', 'play', 'two', 'music', 'ca', 'made', 'colleg', 'even', '``', 'name', 'happi', 'daughter', 'rule', 'sexual', 'tax', 'step', 'bad', 'son', 'poll', '9', 'next', 'job', 'coupl', 'got', 'happen', 'perfect', 'law', 'sander', 'littl', '2013', 'heart', 'north', 'sex', 'pay', 'histori', 'ad', 'polit', 'medium', 'film', 'night', 'becom', 'leav', 'inspir', 'mean', 'gift', 'money', 'hit', 'moment', 'claim', 'last', 'book', 'date', 'letter', 'feel', 'huffpost', 'lesson', '-', 'miss', 'christma', 'problem', 'race', 'much', 'countri', 'super', 'case', 'friend', 'without', 'cop', 'anoth', 'start', 'hotel', 'debat', 'mind', 'fan', '2014', 'surpris', 'risk', 'person', 'offici', 'lose', 'paul', '#', 'california', 'wear', 'boy', '*', 'put', 'leader', 'immigr', 'former', 'lost', 'futur', 'berni', 'tv', \"'ll\", '1', 'accus', 'email', 'around', 'bring', 'street', \"'ve\", 'bodi', 'high', 'healthi', 'found', 'actual', '2016', 'place', 'set', 'design', 'voter', 'creat', 'olymp', 'father', 'award', 'rise', 'rais', 'lead', 'drug', 'model', 'social', 'hair', 'red', 'worst', 'jame', \"'m\", 'michael', 'cover', 'might', 'test', 'sign', 'kim', 'interview', 'fear', 'challeng', 'suprem', 'kardashian', 'free', 'weight', 'fail', 'facebook', 'weekend', 'group', 'explain', 'return', 'car', 'matter', 'young', 'hope', 'age', 'ryan', 'russia', 'compani', 'arrest', 'dream', 'wall', 'releas', 'visit', 'team', 'favorit', 'behind', 'cut', 'south', 'morn', 'stress', 'spring', 'goe', 'protect', 'oscar', 'list', '12', 'congress', 'le', 'commun', 'insid', 'success', 'crisi', 'artist', 'offer', 'propos', 'human', 'head', 'link', 'candid', 'wrong', 'stay', 'charg', 'word', '20', 'texa', 'month', 'muslim', '11', 'violenc', 'buy', 'judg', 'educ', 'water', 'kate', 'victim', 'updat', 'forc', 'public', 'stand', 'medit', 'pari', 'differ', 'obamacar', 'away', 'secur', 'anim', 'hate', 'song', 'easi', 'perform', 'line', 'speech', 'must', '15', 'florida', 'teach', 'critic', 'relationship', 'thank', 'prove', 'post', 'record', 'jimmi', 'issu', 'warn', 'pick', 'china', 'ador', 'respons', 'major', 'order', 'drink', 'presidenti', 'investig', 'marri', 'rock', 'govern', 'read', 'west', 'vs.', 'brain', 'singl', 'long', 'import', 'march', 'bowl', 'birthday', 'trip', 'advic', 'shot', 'chri', 'guy', 'everyth', 'old', 'honor', 'control', 'abus', 'key', 'avoid', 'suggest', 'gener', 'winter', 'second', 'player', 'isi', 'justic', 'hot', 'univers', 'refuge', 'assault', 'polici', 'messag', 'korea', 'defend', 'instagram', 'global', 'cook', 'battl', 'announc', 'cost', 'clean', 'mani', 'teacher', 'build', 'alleg', 'washington']\n"]}]},{"cell_type":"code","source":["text = 'FBI found gold hidden in a old house'\n","bagofwords(text, vocab)"],"metadata":{"trusted":true},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{},"execution_count":8}]},{"source":["## Building the bag of words"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["n_words = len(vocab)\n","n_docs = len(df)\n","bag_o = np.zeros([n_docs,n_words])\n","for ii in range(n_docs): \n","    bag_o[ii,:] = bagofwords(df['headline'].iloc[ii], vocab) "],"metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["bag_o.shape"],"metadata":{"trusted":true},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200853, 500)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["popular_words = sorted(hash_map, key = hash_map.get, reverse = True)\n","print(popular_words[:20])"],"metadata":{"trusted":true},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[',', ':', \"'s\", \"'\", '(', ')', 'trump', 'photo', '?', 'new', \"n't\", 'video', 'say', 'â€™', 'day', 'get', 'make', 'woman', '.', 'donald']\n"]}]},{"source":["## Finding the idf and tfidf"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["numdocs, numwords = np.shape(bag_o)\n","N = numdocs\n","word_frequency = np.empty(numwords)\n","\n","for word in range(numwords):\n","    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n","\n","idf = np.log(N/word_frequency)\n","idf.shape\n","pickle.dump(idf, open('web/idf.pickle', 'wb'))"],"metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["tfidf = np.empty([numdocs, numwords])\n","for doc in range(numdocs):\n","    tfidf[doc, :]=bag_o[doc, :]*idf"],"metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print (tfidf)"],"metadata":{"trusted":true},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.8677155  0.         0.         ... 0.         0.         0.        ]\n [0.         0.         1.8071269  ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [1.8677155  0.         0.         ... 0.         0.         0.        ]\n [0.         1.70737285 0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]]\n"]}]},{"source":["## Code to build the model and export it in a pickle"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression #to import logistic regression model\n","from sklearn.model_selection import train_test_split #to split data into training and testing set\n","from sklearn.model_selection import GridSearchCV #to find out the best parameter for our model"],"metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["X_train,X_test,y_train,y_test = train_test_split(tfidf,df['relevance'].values,shuffle=True)"],"metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"metadata":{"trusted":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(150639, 500)\n(50214, 500)\n(150639,)\n(50214,)\n"]}]},{"cell_type":"code","source":["logreg = LogisticRegression(solver = 'lbfgs')\n","logreg.fit(X_train,y_train)\n","y_pred=logreg.predict(X_test)\n","print (y_pred)\n","score = logreg.score(X_train, y_train)\n","print(score)\n","print('Accuracy of logistic regression classifier on training set: {:.3f}'.format(score))"],"metadata":{"trusted":true},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 0 ... 1 1 1]\n","0.7277464667184461\n","Accuracy of logistic regression classifier on training set: 0.728\n"]}]},{"cell_type":"code","source":["def classify(rf, X_all, y_all): \n","    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True)\n","    logreg.fit(X_train,y_train) \n","    print(rf.score(X_train,y_train)) \n","    return logreg"],"metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["logreg = LogisticRegression()\n","X_all = tfidf\n","y_all = df['relevance'].values\n","logreg = classify(logreg, X_all, y_all)\n","pickle.dump(logreg, open('web/logreg.pickle', 'wb'))"],"metadata":{"trusted":true},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["0.727846042525508\n"]}]}]}